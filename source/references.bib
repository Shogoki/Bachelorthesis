@article{Ouellet2014,
abstract = {The goal of the present study is to explore the application of deep convolutional network features to emotion recognition. Results indicate that they perform similarly to other published models at a best recognition rate of 94.4{\%}, and do so with a single still image rather than a video stream. An implementation of an affective feedback game is also described, where a classifier using these features tracks the facial expressions of a player in real-time.},
archivePrefix = {arXiv},
arxivId = {1408.3750},
author = {Ouellet, S{\'{e}}bastien},
eprint = {1408.3750},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Ouellet - 2014 - Real-time emotion recognition for gaming using deep convolutional network features.pdf:pdf},
keywords = {affective,convolutional network,emotion recognition},
mendeley-groups = {Bachelor},
pages = {1--6},
title = {{Real-time emotion recognition for gaming using deep convolutional network features}},
url = {http://arxiv.org/abs/1408.3750},
year = {2014}
}
@article{Lian2018,
abstract = {Automatic emotion recognition is a challenging task. In this paper, we present our effort for the audio-video based sub-challenge of the Emotion Recognition in the Wild (EmotiW) 2018 challenge, which requires participants to assign a single emotion label to the video clip from the six universal emotions (Anger, Disgust, Fear, Happiness, Sad and Surprise) and Neutral. The proposed multimodal emotion recognition system takes audio, video and text information into account. Except for handcraft features, we also extract bottleneck features from deep neutral networks (DNNs) via transfer learning. Both temporal classifiers and non-temporal classifiers are evaluated to obtain the best unimodal emotion classification result. Then possibilities are extracted and passed into the Beam Search Fusion (BS-Fusion). We test our method in the EmotiW 2018 challenge and we gain promising results. Compared with the baseline system, there is a significant improvement. We achieve 60.34{\%} accuracy on the testing dataset, which is only 1.5{\%} lower than the winner. It shows that our method is very competitive.},
author = {Lian, Zheng and Li, Ya and Tao, Jianhua and Huang, Jian},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Lian et al. - 2018 - Investigation of Multimodal Features, Classifiers and Fusion Methods for Emotion Recognition.pdf:pdf},
keywords = {classifiers,emotion recognition,fusion,multimodal features},
mendeley-groups = {Bachelor},
pages = {1--9},
title = {{Investigation of Multimodal Features, Classifiers and Fusion Methods for Emotion Recognition}},
year = {2018}
}
@article{Wu2006,
abstract = {This study presents a novel approach to automatic emotion recognition from text. First, emotion generation rules (EGRs) are manually deduced from psychology to represent the conditions for generating emotion. Based on the EGRs, the emotional state of each sentence can be represented as a sequence of semantic labels (SLs) and attributes (ATTs); SLs are defined as the domain-independent features, while ATTs are domain-dependent. The emotion association rules (EARs) represented by SLs and ATTs for each emotion are automatically derived from the sentences in an emotional text corpus using the a priori algorithm. Finally, a separable mixture model (SMM) is adopted to estimate the similarity between an input sentence and the EARs of each emotional state. Since some features defined in this approach are domain-dependent, a dialog system focusing on the students' daily expressions is constructed, and only three emotional states, happy, unhappy, and neutral, are considered for performance evaluation. According to the results of the experiments, given the domain corpus, the proposed approach is promising, and easily ported into other domains.},
author = {Wu, Chung-Hsien and Chuang, Ze-Jing and Lin, Yu-Chung},
doi = {10.1145/1165255.1165259},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Chuang, Lin - 2006 - Emotion recognition from text using semantic labels and separable mixture models.pdf:pdf},
issn = {15300226},
journal = {ACM Transactions on Asian Language Information Processing},
mendeley-groups = {Bachelor},
number = {2},
pages = {165--183},
title = {{Emotion recognition from text using semantic labels and separable mixture models}},
volume = {5},
year = {2006}
}
@article{Schuller2003,
author = {Schuller, Bj{\"{o}}rn and Rigoll, Gerhard and Lang, Manfred},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Schuller, Rigoll, Lang - 2003 - Sprachliche Emotionserkennung im Fahrzeug.pdf:pdf},
journal = {DGLR Bericht},
mendeley-groups = {Bachelor},
pages = {227--240},
title = {{Sprachliche Emotionserkennung im Fahrzeug}},
year = {2003}
}
@article{Wallhoff2006,
author = {Wallhoff, Frank},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Wallhoff - 2006 - Entwicklung und Evaluierung neuartiger Verfahren zur automatischen Gesichtsdetektion , Identifikation und Emotionserke.pdf:pdf},
keywords = {Mensch-Maschine-Kommunikation, Gesichtsdetektion,},
mendeley-groups = {Bachelor},
title = {{Entwicklung und Evaluierung neuartiger Verfahren zur automatischen Gesichtsdetektion , Identifikation und Emotionserkennung}},
year = {2006}
}
@book{Verma2017,
abstract = {This chapter introduces a general framework for roadside video data analysis. The main processing steps in the framework are described separately. It also reviews previous related work on vegetation and generic object segmentation, and lists several commonly used data processing algorithms. {\textcopyright} Springer Nature Singapore Pte Ltd. 2017.},
author = {Verma, Brijesh and Zhang, Ligang and Stockwell, David},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-981-10-4539-4_2},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Verma, Zhang, Stockwell - 2017 - Roadside video data analysis framework.pdf:pdf},
isbn = {9789811045387},
issn = {1860949X},
mendeley-groups = {Bachelor},
pages = {13--39},
title = {{Roadside video data analysis framework}},
volume = {711},
year = {2017}
}
@book{Camargo2017,
abstract = {Recent work in image captioning and scene-segmentation has shown significant results in the context of scene-understanding. However, most of these developments have not been extrapolated to research areas such as robotics. In this work we review the current state-ofthe- art models, datasets and metrics in image captioning and scenesegmentation. We introduce an anomaly detection dataset for the purpose of robotic applications, and we present a deep learning architecture that describes and classifies anomalous situations. We report a METEOR score of 16.2 and a classification accuracy of 97 {\%}.},
author = {Camargo, Luis Octavio Arriaga},
doi = {10.18418/978-3-96043-045-2},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Camargo - 2017 - Scene understanding through Deep Learning.pdf:pdf},
isbn = {978-3-96043-045-2},
issn = {1869-5272},
keywords = {Scene understanding through Deep Learning, image c},
mendeley-groups = {Bachelor},
number = {02-2017},
pages = {77},
title = {{Scene understanding through Deep Learning}},
year = {2017}
}
@article{Gupta2013,
author = {Gupta, Meeta Sharma and Reddi, Vijay Janapa},
doi = {10.2200/S00783ED1V01Y201706CAC041},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, Reddi - 2013 - Synthesis Lectures on Computer Architecture.pdf:pdf},
isbn = {9781608456376},
issn = {1935-3235},
mendeley-groups = {Bachelor},
title = {{Synthesis Lectures on Computer Architecture}},
year = {2013}
}
@article{Barsoum2016,
abstract = {Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More specifically, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community.},
archivePrefix = {arXiv},
arxivId = {1608.01041},
author = {Barsoum, Emad and Zhang, Cha and Ferrer, Cristian Canton and Zhang, Zhengyou},
eprint = {1608.01041},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Barsoum et al. - 2016 - Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution.pdf:pdf},
mendeley-groups = {Bachelor},
month = {aug},
title = {{Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution}},
url = {http://arxiv.org/abs/1608.01041},
year = {2016}
}
@article{Schuller2006,
abstract = {derived out of the series by means of descriptive statistics. Additionally, an exhaustive search for the optimal classifier is provided. Among such instance based learning, stochastic modelling, Kernel Machines, Neural Nets, and Decision Trees can be found. The application of ensemble construction techniques such as MultiBoosting or Stacking in order to enhance and combine the individual strengths of base classifiers further supports the improvement in connection with general performance. The following analysis of the spoken content in association with emotional information enhances recognition accuracy, and enables accessory processing of written text. Thereby novel approaches within this field such as Graphical or Vector Space Modelling are compared to classical N-Gram representation. Capturing of the text itself is discussed within a brief digression on Soft-String- Matching, Automatic Handwriting and Speech Recognition. Innovatively, also conventional interaction by a computer-mouse without additional hardware, and the usage of a touch-screen will be considered as further modalities to estimate an underlying user affect. Analogical to acoustic processing relevant attributes and models for the recognition will be introduced and critically evaluated. The processing of single information streams is refined by the introduction of methods for their synergistic integration. Thereby multi-stream and multimodal fusion on a feature and semantic level are dealt with. Considering a real-life application, adaptation to the actual user in order to improve overall accuracy is furthermore performed. Finally, three use-cases are presented, which are Robust Automatic Speech Recognition, multimodal Music Information Retrieval, and affective interaction in an automotive environment. Concluding, emotion can be recognized close to human performance based on acoustic information given ideal conditions, integration of spoken content analysis significantly boosts performance, and estimation out of manual interaction seems generally feasible.},
author = {Schuller, Bj{\"{o}}rn},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Schuller - 2006 - Automatische Emotionserkennung aus sprachlicher und manueller Interaktion.pdf:pdf},
isbn = {978-3-8364-1522-4},
mendeley-groups = {Bachelor},
pages = {243},
title = {{Automatische Emotionserkennung aus sprachlicher und manueller Interaktion}},
year = {2006}
}
@book{Kruse2015,
abstract = {Frequently, when an evolutionary algorithm is applied to a population of symbolic expressions, the shapes of these symbolic expressions are very different at the first generations whereas they become more similar during the evolving process. In fact, when the evolutionary algorithm finishes most of the best symbolic expressions only differ in some of its coefficients. In this paper we present several coevolutionary strategies of a genetic program that evolves symbolic expressions represented by straight line programs and an evolution strategy that searches for good coefficients. The presented methods have been applied to solve instances of symbolic regression problem, corrupted by additive noise. A main contribution of the work is the introduction of a fitness function with a penalty term, besides the well known fitness function based on the empirical error over the sample set. The results show that in the presence of noise, the coevolutionary architecture with penalized fitness function outperforms the strategies where only the empirical error is considered in order to evaluate the symbolic expressions of the population. {\textcopyright} 2012 Springer-Verlag GmbH Berlin Heidelberg.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kruse, Rudolf and Borgelt, Christian and Braune, Christian and Klawonn, Frank and Moewes, Christian and Steinbrecher, Matthias},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-658-10904-2},
eprint = {arXiv:1011.1669v3},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Kruse et al. - 2015 - Computational Intelligence Eine methodische Einf{\"{u}}hrung in K{\"{u}}nstliche Neuronale Netze, Evolution{\"{a}}re Algorithmen,.pdf:pdf},
isbn = {978-3-658-10903-5},
issn = {1098-6596},
keywords = {Coevolution,Genetic Programming,Penalty term,Straight-line Programs,Symbolic Regression},
mendeley-groups = {Bachelor},
pages = {515},
pmid = {25246403},
title = {{Computational Intelligence: Eine methodische Einf{\"{u}}hrung in K{\"{u}}nstliche Neuronale Netze, Evolution{\"{a}}re Algorithmen, Fuzzy-Systeme und Bayes-Netze}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84858020615{\&}partnerID=tZOtx3y1},
year = {2015}
}
@article{News2018,
author = {News},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/News - 2018 - KI - K{\"{u}}nstliche Intelligenz.pdf:pdf},
journal = {Http://Link.Springer.Com/Article/10.1007/S13218-011-0122-Y},
mendeley-groups = {Bachelor},
number = {25},
pages = {10.1007/s13218--011--0122--y},
title = {{KI - K{\"{u}}nstliche Intelligenz}},
year = {2018}
}
@article{Levi2016,
abstract = {We present a novel method for classifying emotions from static facial images. Our approach leverages on the recent success of Convolutional Neural Networks (CNN) on face recognition problems. Unlike the settings often assumed there, far less labeled data is typically available for train-ing emotion classification systems. Our method is therefore designed with the goal of simplifying the problem domain by removing confounding factors from the input images, with an emphasis on image illumination variations. This, in an ef-fort to reduce the amount of data required to effectively train deep CNN models. To this end, we propose novel transfor-mations of image intensities to 3D spaces, designed to be invariant to monotonic photometric transformations. These are applied to CASIA Webface images which are then used to train an ensemble of multiple architecture CNNs on mul-tiple representations. Each model is then fine-tuned with limited emotion labeled training data to obtain final clas-sification models. Our method was tested on the Emotion Recognition in the Wild Challenge (EmotiW 2015), Static Facial Expression Recognition sub-challenge (SFEW) and shown to provide a substantial, 15.36{\%} improvement over baseline results (40{\%} gain in performance).},
author = {Levi, Gil and Hassner, Tal},
doi = {10.1145/2818346.2830587},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Levi, Hassner - 2016 - Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns.pdf:pdf},
isbn = {9781450339834},
keywords = {deep learning,emotion recognition,emotiw 2015 challenge,local binary pat-,terns},
mendeley-groups = {Bachelor},
pages = {503--510},
title = {{Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns}},
year = {2016}
}
@techreport{Rohrer2017,
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Gestenerkennung im Augmented-Reality-Umfeld mittels neuronaler Netze.pdf:pdf},
mendeley-groups = {Bachelor},
title = {{Gestenerkennung im Augmented-Reality-Umfeld mittels neuronaler Netze}}
}
@techreport{Preisler2018,
author = {Preisler, Herr Bernhard and Herr, Referent : and Becker, Christoph and Frau, Korreferentin : and Mieskes, Margot},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Preisler et al. - 2018 - Hochschule Darmstadt Fachbereiche Mathematik und Naturwissenschaften {\&} Informatik Stimmungsquantifizierung f{\"{u}}r.pdf:pdf},
mendeley-groups = {Bachelor},
title = {{Hochschule Darmstadt Fachbereiche Mathematik und Naturwissenschaften {\&} Informatik Stimmungsquantifizierung f{\"{u}}r den Preis von Bitcoin mit Deep Learning}},
year = {2018}
}
@article{Brand2012,
abstract = {Menschen kommunizieren nicht nur durch Sprache miteinander, sondern geben unbe- wusst auch eine Menge Informationen durch Emotionen preis. Da die herk{\"{o}}mmliche Kom- munikation zwischen Mensch und Computer oft nicht sehr intuitiv ist, soll sie durch die Entwicklungen im ForschungsbereichAffective Computing verbessertwerden.Hierf{\"{u}}rmussder Computer in der Lage sein, die Gef{\"{u}}hle seines Gegen{\"{u}}bersrichtig zu deuten. In diesemArtikel werden die Technologien und Verfahren zur Erkennung menschlicher Emotionen von Computersystemen vorge- stellt, die Deutung der Daten zu Emotionen beschriebenund Anwendungsbeispielegezeigt. Der Schwerpunkt liegt bei den technischen Komponenten derEmotionserkennung. Hierbei stehtdie Deutung vonGesichtsz{\"{u}}gen, Ausspra- che und Vitaldaten (Puls, Blutdruck etc.) im Vordergrund. AndereHinweisewie Gestik und K{\"{o}}rperhaltung sind sehrschwer durch Sensorik zu messen bzw. kaum zu interpretieren. Die zuverl{\"{a}}ssigsten Ergebnisseliefern stets multi- modale Verfahren, bei denen mehrereSensoren als Quellen verwendet werden, um eine besser Zuordnung zu einem emotionalen Zustand zu erm{\"{o}}glichen.Diesspiegeltauchdiemenschliche Wahrnehmung wider, da wir Emotionen {\"{u}}ber eineVielzahl vonSinnesorganenwahrnehmen.},
author = {Brand, Marcel and Klompmaker, Florian and Schleining, Peter and Wei{\ss}, Fabian},
doi = {10.1007/s00287-012-0618-3},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Brand et al. - 2012 - Automatische emotionserkennung - technologien, deutung und anwendungen.pdf:pdf},
issn = {01706012},
journal = {Informatik-Spektrum},
mendeley-groups = {Bachelor},
number = {6},
pages = {424--432},
title = {{Automatische emotionserkennung - technologien, deutung und anwendungen}},
volume = {35},
year = {2012}
}
@book{Fakult2007,
author = {Fakult, Von Der and Michael, H},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Fakult, Michael - 2007 - Modellbasierte posen- und mimikinvariante Gesichtserkennung.pdf:pdf},
isbn = {9783899637038},
mendeley-groups = {Bachelor},
number = {November 2007},
title = {{Modellbasierte posen- und mimikinvariante Gesichtserkennung}},
url = {http://darwin.bth.rwth-aachen.de/opus3/volltexte/2008/2240/pdf/Haehnel{\_}Michael.pdf},
year = {2007}
}
@article{Gajarla2015,
abstract = {If we search for a tag "love" on Flickr, we get a wide variety of images: roses, a mother holding her baby, images with hearts, etc. These images are very different from one another and yet depict the same emotion of " love " in them. In this project, we explore the possibility of using deep learning to predict the emotion depicted by an image. Our results look promising and indicate that neural nets are indeed capable of learning the emotion essayed by an image. These kinds of predictions can be used in applications like automatic tag predictions for images uploaded on social media websites and understanding sentiment of people and their mood during/after an election.},
author = {Gajarla, Vasavi},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Gajarla - 2015 - Emotion Detection and Sentiment Analysis of Images Georgia Institute of Technology.pdf:pdf},
journal = {Georgia Institute of Technology},
mendeley-groups = {Bachelor},
title = {{Emotion Detection and Sentiment Analysis of Images Georgia Institute of Technology}},
year = {2015}
}
@article{Konig2017,
abstract = {1},
author = {K{\"{o}}nig, Daniel},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{o}}nig - 2017 - Deep Learning for Person Detection in Multi-Spectral Videos.pdf:pdf},
journal = {Thesis},
mendeley-groups = {Bachelor},
title = {{Deep Learning for Person Detection in Multi-Spectral Videos}},
url = {https://d-nb.info/1135265593/34},
year = {2017}
}
@phdthesis{Schmidt2014,
author = {Schmidt, Prof Albrecht and A, Oliver Korn M},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt, A - 2014 - Prototypische Realisierung mit Betrachtung der ethischen Dimension Pr{\"{u}}fer Betreuer begonnen am beendet am Inhal.pdf:pdf},
mendeley-groups = {Bachelor},
number = {Mci},
title = {{Prototypische Realisierung mit Betrachtung der ethischen Dimension Pr{\"{u}}fer : Betreuer : begonnen am : beendet am : Inhaltsverzeichnis}},
year = {2014}
}
@book{Kirste2018,
author = {Kirste, Moritz and Sch{\"{u}}rholz, Markus},
booktitle = {K{\"{u}}nstliche Intelligenz},
doi = {10.1007/978-3-662-58042-4_1},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Kirste, Sch{\"{u}}rholz - 2018 - Einleitung Entwicklungswege zur KI.pdf:pdf},
isbn = {9783662580417},
mendeley-groups = {Bachelor},
pages = {21--35},
title = {{Einleitung: Entwicklungswege zur KI}},
year = {2018}
}
@article{Damer2018,
author = {Damer, Naser},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Damer - 2018 - Application-driven Advances in Multi-biometric Fusion.pdf:pdf},
mendeley-groups = {Bachelor},
number = {June},
title = {{Application-driven Advances in Multi-biometric Fusion}},
year = {2018}
}
@article{TobiasEppelausCalw2017,
author = {{Tobias Eppel aus Calw}},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Tobias Eppel aus Calw - 2017 - Differentialpsychologische Untersuchung der mimischen Emotionserkennung hinsichtlich der Faktoren Alexit.pdf:pdf},
mendeley-groups = {Bachelor},
number = {6},
pages = {67--72},
title = {{Differentialpsychologische Untersuchung der mimischen Emotionserkennung hinsichtlich der Faktoren Alexithymie, Emotionale Intelligenz, Emotionsregulation und Pers{\"{o}}nlichkeit}},
year = {2017}
}
@book{HeinsohnBoerschSocher2012,
author = {Heinsohn, Jochen and Boersch, Ingo and Socher, Rolf},
edition = {2. Aufl.},
isbn = {978-3-8274-1844-9},
title = {{Wissensverarbeitung : eine Einf{\"{u}}hrung in die K{\"{u}}nstliche Intelligenz f{\"{u}}r Informatiker und Ingenieure}},
year = {2012}
}
