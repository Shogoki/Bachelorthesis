@article{Ouellet2014,
abstract = {The goal of the present study is to explore the application of deep convolutional network features to emotion recognition. Results indicate that they perform similarly to other published models at a best recognition rate of 94.4{\%}, and do so with a single still image rather than a video stream. An implementation of an affective feedback game is also described, where a classifier using these features tracks the facial expressions of a player in real-time.},
archivePrefix = {arXiv},
arxivId = {1408.3750},
author = {Ouellet, S{\'{e}}bastien},
eprint = {1408.3750},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Ouellet - 2014 - Real-time emotion recognition for gaming using deep convolutional network features.pdf:pdf},
keywords = {affective,convolutional network,emotion recognition},
mendeley-groups = {Bachelor},
pages = {1--6},
title = {{Real-time emotion recognition for gaming using deep convolutional network features}},
url = {http://arxiv.org/abs/1408.3750},
year = {2014}
}
@article{Lian2018,
abstract = {Automatic emotion recognition is a challenging task. In this paper, we present our effort for the audio-video based sub-challenge of the Emotion Recognition in the Wild (EmotiW) 2018 challenge, which requires participants to assign a single emotion label to the video clip from the six universal emotions (Anger, Disgust, Fear, Happiness, Sad and Surprise) and Neutral. The proposed multimodal emotion recognition system takes audio, video and text information into account. Except for handcraft features, we also extract bottleneck features from deep neutral networks (DNNs) via transfer learning. Both temporal classifiers and non-temporal classifiers are evaluated to obtain the best unimodal emotion classification result. Then possibilities are extracted and passed into the Beam Search Fusion (BS-Fusion). We test our method in the EmotiW 2018 challenge and we gain promising results. Compared with the baseline system, there is a significant improvement. We achieve 60.34{\%} accuracy on the testing dataset, which is only 1.5{\%} lower than the winner. It shows that our method is very competitive.},
author = {Lian, Zheng and Li, Ya and Tao, Jianhua and Huang, Jian},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Lian et al. - 2018 - Investigation of Multimodal Features, Classifiers and Fusion Methods for Emotion Recognition.pdf:pdf},
keywords = {classifiers,emotion recognition,fusion,multimodal features},
mendeley-groups = {Bachelor},
pages = {1--9},
title = {{Investigation of Multimodal Features, Classifiers and Fusion Methods for Emotion Recognition}},
year = {2018}
}
@article{Wu2006,
abstract = {This study presents a novel approach to automatic emotion recognition from text. First, emotion generation rules (EGRs) are manually deduced from psychology to represent the conditions for generating emotion. Based on the EGRs, the emotional state of each sentence can be represented as a sequence of semantic labels (SLs) and attributes (ATTs); SLs are defined as the domain-independent features, while ATTs are domain-dependent. The emotion association rules (EARs) represented by SLs and ATTs for each emotion are automatically derived from the sentences in an emotional text corpus using the a priori algorithm. Finally, a separable mixture model (SMM) is adopted to estimate the similarity between an input sentence and the EARs of each emotional state. Since some features defined in this approach are domain-dependent, a dialog system focusing on the students' daily expressions is constructed, and only three emotional states, happy, unhappy, and neutral, are considered for performance evaluation. According to the results of the experiments, given the domain corpus, the proposed approach is promising, and easily ported into other domains.},
author = {Wu, Chung-Hsien and Chuang, Ze-Jing and Lin, Yu-Chung},
doi = {10.1145/1165255.1165259},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Chuang, Lin - 2006 - Emotion recognition from text using semantic labels and separable mixture models.pdf:pdf},
issn = {15300226},
journal = {ACM Transactions on Asian Language Information Processing},
mendeley-groups = {Bachelor},
number = {2},
pages = {165--183},
title = {{Emotion recognition from text using semantic labels and separable mixture models}},
volume = {5},
year = {2006}
}
@article{Schuller2003,
author = {Schuller, Bj{\"{o}}rn and Rigoll, Gerhard and Lang, Manfred},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Schuller, Rigoll, Lang - 2003 - Sprachliche Emotionserkennung im Fahrzeug.pdf:pdf},
journal = {DGLR Bericht},
mendeley-groups = {Bachelor},
pages = {227--240},
title = {{Sprachliche Emotionserkennung im Fahrzeug}},
year = {2003}
}
@article{Wallhoff2006,
author = {Wallhoff, Frank},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Wallhoff - 2006 - Entwicklung und Evaluierung neuartiger Verfahren zur automatischen Gesichtsdetektion , Identifikation und Emotionserke.pdf:pdf},
keywords = {Mensch-Maschine-Kommunikation, Gesichtsdetektion,},
mendeley-groups = {Bachelor},
title = {{Entwicklung und Evaluierung neuartiger Verfahren zur automatischen Gesichtsdetektion , Identifikation und Emotionserkennung}},
year = {2006}
}
@book{Verma2017,
abstract = {This chapter introduces a general framework for roadside video data analysis. The main processing steps in the framework are described separately. It also reviews previous related work on vegetation and generic object segmentation, and lists several commonly used data processing algorithms. {\textcopyright} Springer Nature Singapore Pte Ltd. 2017.},
author = {Verma, Brijesh and Zhang, Ligang and Stockwell, David},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-981-10-4539-4_2},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Verma, Zhang, Stockwell - 2017 - Roadside video data analysis framework.pdf:pdf},
isbn = {9789811045387},
issn = {1860949X},
mendeley-groups = {Bachelor},
pages = {13--39},
title = {{Roadside video data analysis framework}},
volume = {711},
year = {2017}
}
@book{Camargo2017,
abstract = {Recent work in image captioning and scene-segmentation has shown significant results in the context of scene-understanding. However, most of these developments have not been extrapolated to research areas such as robotics. In this work we review the current state-ofthe- art models, datasets and metrics in image captioning and scenesegmentation. We introduce an anomaly detection dataset for the purpose of robotic applications, and we present a deep learning architecture that describes and classifies anomalous situations. We report a METEOR score of 16.2 and a classification accuracy of 97 {\%}.},
author = {Camargo, Luis Octavio Arriaga},
doi = {10.18418/978-3-96043-045-2},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Camargo - 2017 - Scene understanding through Deep Learning.pdf:pdf},
isbn = {978-3-96043-045-2},
issn = {1869-5272},
keywords = {Scene understanding through Deep Learning, image c},
mendeley-groups = {Bachelor},
number = {02-2017},
pages = {77},
title = {{Scene understanding through Deep Learning}},
year = {2017}
}
@article{Gupta2013,
author = {Gupta, Meeta Sharma and Reddi, Vijay Janapa},
doi = {10.2200/S00783ED1V01Y201706CAC041},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, Reddi - 2013 - Synthesis Lectures on Computer Architecture.pdf:pdf},
isbn = {9781608456376},
issn = {1935-3235},
mendeley-groups = {Bachelor},
title = {{Synthesis Lectures on Computer Architecture}},
year = {2013}
}
@article{Barsoum2016,
abstract = {Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More specifically, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community.},
archivePrefix = {arXiv},
arxivId = {1608.01041},
author = {Barsoum, Emad and Zhang, Cha and Ferrer, Cristian Canton and Zhang, Zhengyou},
eprint = {1608.01041},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Barsoum et al. - 2016 - Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution.pdf:pdf},
mendeley-groups = {Bachelor},
month = {aug},
title = {{Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution}},
url = {http://arxiv.org/abs/1608.01041},
year = {2016}
}
@article{Schuller2006,
abstract = {derived out of the series by means of descriptive statistics. Additionally, an exhaustive search for the optimal classifier is provided. Among such instance based learning, stochastic modelling, Kernel Machines, Neural Nets, and Decision Trees can be found. The application of ensemble construction techniques such as MultiBoosting or Stacking in order to enhance and combine the individual strengths of base classifiers further supports the improvement in connection with general performance. The following analysis of the spoken content in association with emotional information enhances recognition accuracy, and enables accessory processing of written text. Thereby novel approaches within this field such as Graphical or Vector Space Modelling are compared to classical N-Gram representation. Capturing of the text itself is discussed within a brief digression on Soft-String- Matching, Automatic Handwriting and Speech Recognition. Innovatively, also conventional interaction by a computer-mouse without additional hardware, and the usage of a touch-screen will be considered as further modalities to estimate an underlying user affect. Analogical to acoustic processing relevant attributes and models for the recognition will be introduced and critically evaluated. The processing of single information streams is refined by the introduction of methods for their synergistic integration. Thereby multi-stream and multimodal fusion on a feature and semantic level are dealt with. Considering a real-life application, adaptation to the actual user in order to improve overall accuracy is furthermore performed. Finally, three use-cases are presented, which are Robust Automatic Speech Recognition, multimodal Music Information Retrieval, and affective interaction in an automotive environment. Concluding, emotion can be recognized close to human performance based on acoustic information given ideal conditions, integration of spoken content analysis significantly boosts performance, and estimation out of manual interaction seems generally feasible.},
author = {Schuller, Bj{\"{o}}rn},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Schuller - 2006 - Automatische Emotionserkennung aus sprachlicher und manueller Interaktion.pdf:pdf},
isbn = {978-3-8364-1522-4},
mendeley-groups = {Bachelor},
pages = {243},
title = {{Automatische Emotionserkennung aus sprachlicher und manueller Interaktion}},
year = {2006}
}
@book{Kruse2015,
abstract = {Frequently, when an evolutionary algorithm is applied to a population of symbolic expressions, the shapes of these symbolic expressions are very different at the first generations whereas they become more similar during the evolving process. In fact, when the evolutionary algorithm finishes most of the best symbolic expressions only differ in some of its coefficients. In this paper we present several coevolutionary strategies of a genetic program that evolves symbolic expressions represented by straight line programs and an evolution strategy that searches for good coefficients. The presented methods have been applied to solve instances of symbolic regression problem, corrupted by additive noise. A main contribution of the work is the introduction of a fitness function with a penalty term, besides the well known fitness function based on the empirical error over the sample set. The results show that in the presence of noise, the coevolutionary architecture with penalized fitness function outperforms the strategies where only the empirical error is considered in order to evaluate the symbolic expressions of the population. {\textcopyright} 2012 Springer-Verlag GmbH Berlin Heidelberg.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kruse, Rudolf and Borgelt, Christian and Braune, Christian and Klawonn, Frank and Moewes, Christian and Steinbrecher, Matthias},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-658-10904-2},
eprint = {arXiv:1011.1669v3},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Kruse et al. - 2015 - Computational Intelligence Eine methodische Einf{\"{u}}hrung in K{\"{u}}nstliche Neuronale Netze, Evolution{\"{a}}re Algorithmen,.pdf:pdf},
isbn = {978-3-658-10903-5},
issn = {1098-6596},
keywords = {Coevolution,Genetic Programming,Penalty term,Straight-line Programs,Symbolic Regression},
mendeley-groups = {Bachelor},
pages = {515},
pmid = {25246403},
title = {{Computational Intelligence: Eine methodische Einf{\"{u}}hrung in K{\"{u}}nstliche Neuronale Netze, Evolution{\"{a}}re Algorithmen, Fuzzy-Systeme und Bayes-Netze}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84858020615{\&}partnerID=tZOtx3y1},
year = {2015}
}
@article{News2018,
author = {Zimmermann, Roland},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/News - 2018 - KI - K{\"{u}}nstliche Intelligenz.pdf:pdf},
journal = {Http://Link.Springer.Com/Article/10.1007/S13218-011-0122-Y},
mendeley-groups = {Bachelor},
number = {25},
pages = {10.1007/s13218--011--0122--y},
title = {{KI - K{\"{u}}nstliche Intelligenz}},
year = {2018}
}
@article{Levi2016,
abstract = {We present a novel method for classifying emotions from static facial images. Our approach leverages on the recent success of Convolutional Neural Networks (CNN) on face recognition problems. Unlike the settings often assumed there, far less labeled data is typically available for train-ing emotion classification systems. Our method is therefore designed with the goal of simplifying the problem domain by removing confounding factors from the input images, with an emphasis on image illumination variations. This, in an ef-fort to reduce the amount of data required to effectively train deep CNN models. To this end, we propose novel transfor-mations of image intensities to 3D spaces, designed to be invariant to monotonic photometric transformations. These are applied to CASIA Webface images which are then used to train an ensemble of multiple architecture CNNs on mul-tiple representations. Each model is then fine-tuned with limited emotion labeled training data to obtain final clas-sification models. Our method was tested on the Emotion Recognition in the Wild Challenge (EmotiW 2015), Static Facial Expression Recognition sub-challenge (SFEW) and shown to provide a substantial, 15.36{\%} improvement over baseline results (40{\%} gain in performance).},
author = {Levi, Gil and Hassner, Tal},
doi = {10.1145/2818346.2830587},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Levi, Hassner - 2016 - Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns.pdf:pdf},
isbn = {9781450339834},
keywords = {deep learning,emotion recognition,emotiw 2015 challenge,local binary pat-,terns},
mendeley-groups = {Bachelor},
pages = {503--510},
title = {{Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns}},
year = {2016}
}
@article{Brand2012,
abstract = {Menschen kommunizieren nicht nur durch Sprache miteinander, sondern geben unbe- wusst auch eine Menge Informationen durch Emotionen preis. Da die herk{\"{o}}mmliche Kom- munikation zwischen Mensch und Computer oft nicht sehr intuitiv ist, soll sie durch die Entwicklungen im ForschungsbereichAffective Computing verbessertwerden.Hierf{\"{u}}rmussder Computer in der Lage sein, die Gef{\"{u}}hle seines Gegen{\"{u}}bersrichtig zu deuten. In diesemArtikel werden die Technologien und Verfahren zur Erkennung menschlicher Emotionen von Computersystemen vorge- stellt, die Deutung der Daten zu Emotionen beschriebenund Anwendungsbeispielegezeigt. Der Schwerpunkt liegt bei den technischen Komponenten derEmotionserkennung. Hierbei stehtdie Deutung vonGesichtsz{\"{u}}gen, Ausspra- che und Vitaldaten (Puls, Blutdruck etc.) im Vordergrund. AndereHinweisewie Gestik und K{\"{o}}rperhaltung sind sehrschwer durch Sensorik zu messen bzw. kaum zu interpretieren. Die zuverl{\"{a}}ssigsten Ergebnisseliefern stets multi- modale Verfahren, bei denen mehrereSensoren als Quellen verwendet werden, um eine besser Zuordnung zu einem emotionalen Zustand zu erm{\"{o}}glichen.Diesspiegeltauchdiemenschliche Wahrnehmung wider, da wir Emotionen {\"{u}}ber eineVielzahl vonSinnesorganenwahrnehmen.},
author = {Brand, Marcel and Klompmaker, Florian and Schleining, Peter and Wei{\ss}, Fabian},
doi = {10.1007/s00287-012-0618-3},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Brand et al. - 2012 - Automatische emotionserkennung - technologien, deutung und anwendungen.pdf:pdf},
issn = {01706012},
journal = {Informatik-Spektrum},
mendeley-groups = {Bachelor},
number = {6},
pages = {424--432},
title = {{Automatische emotionserkennung - technologien, deutung und anwendungen}},
volume = {35},
year = {2012}
}
@book{Fakult2007,
author = {Fakult, Von Der and Michael, H},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Fakult, Michael - 2007 - Modellbasierte posen- und mimikinvariante Gesichtserkennung.pdf:pdf},
isbn = {9783899637038},
mendeley-groups = {Bachelor},
number = {November 2007},
title = {{Modellbasierte posen- und mimikinvariante Gesichtserkennung}},
url = {http://darwin.bth.rwth-aachen.de/opus3/volltexte/2008/2240/pdf/Haehnel{\_}Michael.pdf},
year = {2007}
}
@article{Gajarla2015,
abstract = {If we search for a tag "love" on Flickr, we get a wide variety of images: roses, a mother holding her baby, images with hearts, etc. These images are very different from one another and yet depict the same emotion of " love " in them. In this project, we explore the possibility of using deep learning to predict the emotion depicted by an image. Our results look promising and indicate that neural nets are indeed capable of learning the emotion essayed by an image. These kinds of predictions can be used in applications like automatic tag predictions for images uploaded on social media websites and understanding sentiment of people and their mood during/after an election.},
author = {Gajarla, Vasavi},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Gajarla - 2015 - Emotion Detection and Sentiment Analysis of Images Georgia Institute of Technology.pdf:pdf},
journal = {Georgia Institute of Technology},
mendeley-groups = {Bachelor},
title = {{Emotion Detection and Sentiment Analysis of Images Georgia Institute of Technology}},
year = {2015}
}
@article{Konig2017,
abstract = {1},
author = {K{\"{o}}nig, Daniel},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{o}}nig - 2017 - Deep Learning for Person Detection in Multi-Spectral Videos.pdf:pdf},
journal = {Thesis},
mendeley-groups = {Bachelor},
title = {{Deep Learning for Person Detection in Multi-Spectral Videos}},
url = {https://d-nb.info/1135265593/34},
year = {2017}
}
@phdthesis{Schmidt2014,
author = {Schmidt, Prof Albrecht and A, Oliver Korn M},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt, A - 2014 - Prototypische Realisierung mit Betrachtung der ethischen Dimension Pr{\"{u}}fer Betreuer begonnen am beendet am Inhal.pdf:pdf},
mendeley-groups = {Bachelor},
number = {Mci},
title = {{Prototypische Realisierung mit Betrachtung der ethischen Dimension Pr{\"{u}}fer : Betreuer : begonnen am : beendet am : Inhaltsverzeichnis}},
year = {2014}
}
@book{Kirste2018,
author = {Kirste, Moritz and Sch{\"{u}}rholz, Markus},
booktitle = {K{\"{u}}nstliche Intelligenz},
doi = {10.1007/978-3-662-58042-4_1},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Kirste, Sch{\"{u}}rholz - 2018 - Einleitung Entwicklungswege zur KI.pdf:pdf},
isbn = {9783662580417},
mendeley-groups = {Bachelor},
pages = {21--35},
title = {{Einleitung: Entwicklungswege zur KI}},
year = {2018}
}
@article{Damer2018,
author = {Damer, Naser},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Damer - 2018 - Application-driven Advances in Multi-biometric Fusion.pdf:pdf},
mendeley-groups = {Bachelor},
number = {June},
title = {{Application-driven Advances in Multi-biometric Fusion}},
year = {2018}
}
@article{TobiasEppelausCalw2017,
author = {{Tobias Eppel aus Calw}},
file = {:Users/sven/Library/Application Support/Mendeley Desktop/Downloaded/Tobias Eppel aus Calw - 2017 - Differentialpsychologische Untersuchung der mimischen Emotionserkennung hinsichtlich der Faktoren Alexit.pdf:pdf},
mendeley-groups = {Bachelor},
number = {6},
pages = {67--72},
title = {{Differentialpsychologische Untersuchung der mimischen Emotionserkennung hinsichtlich der Faktoren Alexithymie, Emotionale Intelligenz, Emotionsregulation und Pers{\"{o}}nlichkeit}},
year = {2017}
}
@book{HeinsohnBoerschSocher2012,
author = {Heinsohn, Jochen and Boersch, Ingo and Socher, Rolf},
edition = {2. Aufl.},
isbn = {978-3-8274-1844-9},
title = {{Wissensverarbeitung : eine Einf{\"{u}}hrung in die K{\"{u}}nstliche Intelligenz f{\"{u}}r Informatiker und Ingenieure}},
year = {2012}
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@misc{NeuronalesNetz-de-Hebb,
booktitle = {Hebb Regel},
mendeley-groups = {Bachelor},
pages = {1},
title = {{neuronalesnetz.de - Hebb Regel}},
url = {http://www.neuronalesnetz.de/hebb.html},
urldate = {2019-06-03},
year = {2019},
author={neuronalesnetz.de}
}
@misc{NeuronalesNetz-backProp,
author = {Neuronalesnetz.de},
title = {{Backpropagation}},
urldate = {2019-06-07},
author={neuronalesnetz.de}
}
@misc{Ng2017,
author = {Ng, Andrew},
publisher = {Youtube.com},
title = {{Neural Networks and Deep Learning}},
url = {https://www.youtube.com/watch?v=fXOsFF95ifk{\&}list=PLkDaE6sCZn6Ec-XTbcX1uRg2{\_}u4xOEky0{\&}index=26{\&}t=0s},
year = {2017}
}
@article{CharlesDarwin1872,
author = {{Charles Darwin}, By and {Society Hardwicke}, Ray and {Fritz Muller}, By},
file = {:Users/sven/Downloads/Darwin{\_}1872{\_}expression{\_}of{\_}emotion.pdf:pdf},
keywords = {http://www.archive.org/details/expressionofemot187},
pages = {1862},
title = {{With illustrations. 2 vols. 8vo. 24s. Murray, 1871. THE VARIATION OF ANIMALS AND PLANTS UNDER DOMESTICATION. Third Thousand. With Illustrations. 2 vols. 8vo. 28s. Murray, 1868. Effects of Crossing. With Woodcuts}},
url = {https://pure.mpg.de/rest/items/item{\_}2309885/component/file{\_}2309884/content},
year = {1872}
}
@article{Shen1997,
abstract = {Three recent articles [T.D. Pope, M. Vos, H.T. Tang, K. Griffiths, I.V. Mitchell, P.R. Norton, W. Liu, Y.S. Li, I. Stensgaard, E. L{\ae}gsgaard, F. Besenbacher, Surf. Sci. 337 (1995) 79; J. Yao, Y.G. Shen, D.J. O'Connor, B.V. King, Surf. Sci. 359 (1996) 65; P.W. Murray, I. Stensgaard, E. L{\ae}gsgaard, F. Besenbacher, Surf. Sci. 365 (1996) 591] have attempted to determine the surface composition and structure of the (2 × 2)p4g phase induced by one monolayer (ML) Pd deposition at room temperature on a Cu(001) surface. In order to remove inconsistencies arising from previous studies, the 1 ML Pd/Cu(001) surface has been reinvestigated using low-energy ion scattering (LEIS), low-energy electron diffraction (LEED), three-dimensional (3D) classical scattering simulations and embedded-atom method (EAM) calculations. Using Li+ and He+ ion scattering with calibration measurements on reference samples of Cu(001) and Pd(001), the surface Pd composition of the 1 ML phase was independently determined to be 53 ± 4{\%} and 51 ± 3{\%}, respectively. The results obtained using K+ ion scattering, LEED and computer simulations are consistent with a surface structure which consists of 30{\%} of a clock-rotated (001) Pd phase and 70{\%} of an unreconstructed p(2 × 2) Cu-Pd phase. The stability of the (2 × 2)p4g phase has been confirmed by EAM calculations. {\textcopyright} 1997 Elsevier Science B.V.},
author = {Shen, Y. G. and Bili{\'{c}}, A. and O'Connor, D. J. and King, B. V.},
doi = {10.1016/S0039-6028(97)00710-3},
file = {:Users/sven/Downloads/viola-cvpr-01.pdf:pdf},
issn = {00396028},
journal = {Surface Science},
keywords = {Alloys,Copper,Growth,Low energy ion scattering,Low index single crystal surfaces,Palladium,Surface structure},
number = {1-3},
title = {{Rapid Object Detection using a Boosted Cascade of Simple Features}},
volume = {394},
year = {1997}
}
@article{Ekman1972,
author = {Ekman, Paul},
file = {:Users/sven/Desktop/Ekman, 1997 - Expression or communication.pdf:pdf},
number = {4},
pages = {1--15},
title = {{Ekman-ShouldWeCallIt}},
url = {papers2://publication/uuid/2979D086-4479-4CC8-B669-E4B8E5FC3F74},
volume = {10},
year = {2004}
}
@article{Zhang2018,
abstract = {Appearance-based gaze estimation is promising for unconstrained real-world seeings, but the significant variability in head pose and user-camera distance poses significant challenges for training generic gaze estimators. Data normalization was proposed to can-cel out this geometric variability by mapping input images and gaze labels to a normalized space. Although used successfully in prior works, the role and importance of data normalization remains unclear. To fill this gap, we study data normalization for the first time using principled evaluations on both simulated and real data. We propose a modification to the current data normalization for-mulation by removing the scaling factor and show that our new formulation performs significantly beeer (between 9.5{\%} and 32.7{\%}) in the different evaluation seeings. Using images synthesized from a 3D face model, we demonstrate the benefit of data normalization for the efficiency of the model training. Experiments on real-world images confirm the advantages of data normalization in terms of gaze estimation performance.},
author = {Zhang, Xucong and Sugano, Yusuke and Bulling, Andreas},
doi = {10.1145/3204493.3204548},
file = {:Users/sven/Downloads/zhang18{\_}etra.pdf:pdf},
isbn = {9781450357067},
keywords = {- Computing methodologies  -{\textgreater} Computer vision,- Human-centered computing  -{\textgreater} Pointing,appearance-based gaze estimation,eye tracking,machine learn-},
pages = {1--9},
title = {{Revisiting data normalization for appearance-based gaze estimation}},
year = {2018}
}
@book{gonzalez2008digital,
  title={Digital Image Processing},
  author={Gonzalez, R.C. and Woods, R.E.},
  isbn={9780131687288},
  lccn={2009289249},
  url={https://books.google.de/books?id=8uGOnjRGEzoC},
  year={2008},
  publisher={Pearson/Prentice Hall}
}
@article{posner_russell_peterson_2005, title={The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology}, volume={17}, DOI={10.1017/S0954579405050340}, number={3}, journal={Development and Psychopathology}, publisher={Cambridge University Press}, author={POSNER, JONATHAN and RUSSELL, JAMES A. and PETERSON, BRADLEY S.}, year={2005}, pages={715–734}}
@article{Papageorgiou,
author = {Papageorgiou, Constantine P},
pages = {555--562},
title = {{Haar-Like Features}}
}
@article{Chollet2017,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.02357v3},
author = {Chollet, Fran{\c{c}}ois},
doi = {10.1109/CVPR.2017.195},
eprint = {arXiv:1610.02357v3},
file = {:Users/sven/Desktop/xception.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {1800--1807},
title = {{Xception: Deep learning with depthwise separable convolutions}},
volume = {2017-January},
year = {2017}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:Users/sven/Desktop/1412.6980v8.pdf:pdf},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@misc{Keras.io2019,
author = {Keras.io},
title = {{Keras Applications}},
url = {https://keras.io/applications/},
urldate = {01.07.2019},
year = {2019}
}
@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}
@misc{scikit-learn,
title = {{Scikit-Learn}},
url = {https://scikit-learn.org/stable/{\#}},
urldate = {01.07.2019},
author={scikit-learn.org}
}
@article{Litomisky2012,
abstract = {In November 2010, Microsoft released the Kinect RGB-D sensor as a new Natural User Interface (NUI) for its XBOX 360 gaming platform. The Kinect, like other RGB-D sensors, provides color information as well as the estimated depth for each pixel. At {\$}150, the ...\backslashn},
author = {Litomisky, Krystof},
journal = {Rapport technique},
title = {{Consumer rgb-d cameras and their applications}},
year = {2012}
}
@misc{OpenAPI,
title = {{Openapi}},
url = {https://swagger.io/specification/},
urldate = {01.07.2019},
author={OpenAPI}
}
@misc{Base64,
title = {{Base64}},
url = {https://developer.mozilla.org/en-US/docs/Web/API/WindowBase64/Base64{\_}encoding{\_}and{\_}decoding},
urldate = {01.07.2019},
author={Mozilla Foundation}
}
@misc{Docker,
title = {{Docker}},
url = {https://www.docker.com},
urldate = {02.07.2019},
author={Docker Inc.}
}
